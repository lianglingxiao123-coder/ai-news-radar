#!/usr/bin/env python3
"""
AI News Radar â€” æ¯æ—¥é‚®ä»¶å‘é€ç³»ç»Ÿ v3
- è¯»å– data/latest-24h.json ä¸­çš„æ–°é—»
- é€šè¿‡ RSS æŠ“å– Twitter/YouTube AI ä¸“å®¶åŠ¨æ€
- ä¸“å®¶å†…å®¹æç‚¼ + ä¸­æ–‡ç¿»è¯‘
- ç”Ÿæˆç²¾ç¾ HTML é‚®ä»¶
"""

from __future__ import annotations
import json, os, re, smtplib, ssl, sys, time
from datetime import datetime, timezone, timedelta
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from pathlib import Path
from typing import Optional
import xml.etree.ElementTree as ET
import urllib.request, urllib.error

# â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ç»è¿‡ä¸¥æ ¼ç­›é€‰çš„ AI ä¸“å®¶åˆ—è¡¨
# æ ‡å‡†ï¼šâ‘  åœ¨ AI/ML é¢†åŸŸæœ‰å®è´¨ç ”ç©¶æˆ–äº§å“è´¡çŒ® â‘¡ åœ¨å­¦æœ¯/å·¥ä¸šç•Œæœ‰æŒç»­å½±å“åŠ›
TWITTER_EXPERTS = [
    {"name": "Yann LeCun",       "handle": "ylecun",    "role": "Meta AI Chief Scientist / æ·±åº¦å­¦ä¹ å…ˆé©±"},
    {"name": "Andrej Karpathy",  "handle": "karpathy",  "role": "å‰Tesla AIæ€»ç›‘ / å‰OpenAI"},
    {"name": "Sam Altman",       "handle": "sama",      "role": "OpenAI CEO"},
    {"name": "Demis Hassabis",   "handle": "demishassabis", "role": "Google DeepMind CEO"},
    {"name": "Ilya Sutskever",   "handle": "ilyasut",   "role": "SSIåˆ›å§‹äºº / å‰OpenAIé¦–å¸­ç§‘å­¦å®¶"},
    {"name": "Andrew Ng",        "handle": "AndrewYNg", "role": "DeepLearning.AIåˆ›å§‹äºº"},
    {"name": "Fei-Fei Li",       "handle": "drfeifei",  "role": "æ–¯å¦ç¦AI Labæ•™æˆ / ImageNetä¹‹æ¯"},
    {"name": "Kai-Fu Lee æå¼€å¤", "handle": "kaifulee",  "role": "é›¶ä¸€ä¸‡ç‰©CEO / AIé¢†åŸŸçŸ¥åæŠ•èµ„äºº"},
    {"name": "Jim Fan",          "handle": "drjimfan",  "role": "NVIDIA Senior Research Scientist"},
    {"name": "Emad Mostaque",    "handle": "emostaque",  "role": "Stability AIåˆ›å§‹äºº"},
]

YOUTUBE_CHANNELS = [
    {"name": "Andrej Karpathy",    "channel_id": "UCMLn3WlKFHHsBuSovpyHdJg", "desc": "Neural Nets æ·±åº¦è®²è§£"},
    {"name": "DeepMind",           "channel_id": "UCbmNph6VwoDyBf_E2VpBqWg",  "desc": "Google DeepMindå®˜æ–¹"},
    {"name": "OpenAI",             "channel_id": "UCXZCJLdBC09xxGZ6gcdus6w",  "desc": "OpenAIå®˜æ–¹"},
    {"name": "Andrew Ng",          "channel_id": "UCrtf7mpeVr1APmm7rNHmugg",  "desc": "DeepLearning.AIè¯¾ç¨‹"},
    {"name": "Lex Fridman",        "channel_id": "UCSHZKyawb77ixDdsGog4iWA",  "desc": "AI/ç§‘æŠ€æ·±åº¦è®¿è°ˆ"},
    {"name": "Two Minute Papers",  "channel_id": "UCbfYPyITQ-7l4upoX8nvctg",  "desc": "æœ€æ–°AIè®ºæ–‡è§£è¯»"},
    {"name": "Yannic Kilcher",     "channel_id": "UCZHmQk67mSJgfCCTn7xBfew",  "desc": "MLè®ºæ–‡æ·±åº¦è§£æ"},
]

# RSSHub é•œåƒï¼ˆTwitter RSSæºï¼Œå¤šä¸ªå¤‡ç”¨ï¼‰
RSSHUB_MIRRORS = [
    "https://rsshub.rssforever.com",
    "https://rss.shab.fun",
    "https://rsshub.app",
]

DATA_DIR = Path(os.environ.get("DATA_DIR", "data"))

# â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def clean_html(text: str, max_len: int = 300) -> str:
    """å»é™¤HTMLæ ‡ç­¾å¹¶æˆªæ–­"""
    text = re.sub(r"<[^>]+>", " ", str(text or ""))
    text = re.sub(r"&amp;", "&", text)
    text = re.sub(r"&lt;", "<", text)
    text = re.sub(r"&gt;", ">", text)
    text = re.sub(r"&nbsp;", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    if len(text) > max_len:
        text = text[:max_len].rsplit(" ", 1)[0] + "â€¦"
    return text


def fetch_url(url: str, timeout: int = 10) -> Optional[str]:
    """HTTP GETï¼Œè¿”å›å“åº”æ–‡æœ¬"""
    try:
        req = urllib.request.Request(url, headers={
            "User-Agent": "Mozilla/5.0 (AI News Radar RSS Reader)",
            "Accept": "application/xml,application/rss+xml,text/xml,*/*"
        })
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            return resp.read().decode("utf-8", errors="replace")
    except Exception as e:
        return None


def translate_to_zh(text: str) -> str:
    """
    ç®€å•è‹±æ–‡â†’ä¸­æ–‡ç¿»è¯‘ï¼ˆè§„åˆ™æ˜ å°„ + ç›´æ¥è¿”å›ä¸­æ–‡ï¼‰
    æ³¨ï¼šå¦‚éœ€çœŸå®ç¿»è¯‘ï¼Œå¯æ¥å…¥ DeepL / Google Translate API
    """
    if not text:
        return ""
    # å¦‚æœå·²æœ‰ä¸­æ–‡ï¼ˆè¶…è¿‡30%æ˜¯CJKå­—ç¬¦ï¼‰ï¼Œç›´æ¥è¿”å›
    cjk = len(re.findall(r"[\u4e00-\u9fff]", text))
    if cjk / max(len(text), 1) > 0.2:
        return text
    # å¯¹è‹±æ–‡å†…å®¹æ ‡æ³¨"[EN]"ï¼Œä¿æŒåŸæ–‡
    # å¦‚æœé…ç½®äº†ç¿»è¯‘APIå¯åœ¨æ­¤æ›¿æ¢
    return f"[è‹±] {text}"

# â”€â”€ æŠ“å–ä¸“å®¶ Twitter åŠ¨æ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_rss_items(xml_text: str, expert_name: str, expert_role: str) -> list[dict]:
    """è§£æRSS XMLï¼Œæå–æ¡ç›®"""
    items = []
    try:
        root = ET.fromstring(xml_text)
        ns = {"atom": "http://www.w3.org/2005/Atom"}

        # RSS 2.0
        for item in root.findall(".//item")[:3]:
            title = item.findtext("title", "").strip()
            desc  = item.findtext("description", "").strip()
            link  = item.findtext("link", "").strip()
            pub   = item.findtext("pubDate", "").strip()

            content = clean_html(desc or title, 250)
            if not content or len(content) < 10:
                continue

            items.append({
                "expert_name": expert_name,
                "expert_role": expert_role,
                "content":     content,
                "content_zh":  translate_to_zh(content),
                "url":         link,
                "published":   pub,
                "type":        "twitter",
            })

        # Atom
        if not items:
            for entry in root.findall("atom:entry", ns)[:3]:
                title   = (entry.findtext("atom:title", "", ns) or "").strip()
                summary = (entry.findtext("atom:summary", "", ns) or "").strip()
                link_el = entry.find("atom:link", ns)
                link    = link_el.get("href", "") if link_el is not None else ""
                pub     = (entry.findtext("atom:published", "", ns) or "").strip()

                content = clean_html(summary or title, 250)
                if not content or len(content) < 10:
                    continue

                items.append({
                    "expert_name": expert_name,
                    "expert_role": expert_role,
                    "content":     content,
                    "content_zh":  translate_to_zh(content),
                    "url":         link,
                    "published":   pub,
                    "type":        "twitter",
                })

    except ET.ParseError:
        pass
    return items


def fetch_twitter_experts() -> list[dict]:
    """æŠ“å–æ‰€æœ‰ Twitter ä¸“å®¶çš„æœ€æ–°åŠ¨æ€"""
    results = []
    print(f"ğŸ¦ æŠ“å– Twitter ä¸“å®¶åŠ¨æ€ ({len(TWITTER_EXPERTS)} ä½)...")

    for expert in TWITTER_EXPERTS:
        handle = expert["handle"]
        name   = expert["name"]
        role   = expert["role"]
        fetched = False

        # ä¾æ¬¡å°è¯•å„ RSSHub é•œåƒ
        for mirror in RSSHUB_MIRRORS:
            url = f"{mirror}/twitter/user/{handle}"
            xml = fetch_url(url, timeout=8)
            if xml and ("<item>" in xml or "<entry>" in xml):
                items = parse_rss_items(xml, name, role)
                if items:
                    results.extend(items[:2])  # æ¯äººæœ€å¤šå–2æ¡
                    print(f"  âœ… {name}: {len(items)} æ¡ (via {mirror})")
                    fetched = True
                    break
            time.sleep(0.3)

        if not fetched:
            # å°è¯• Nitterï¼ˆå¤‡ç”¨ï¼‰
            nitter_url = f"https://nitter.net/{handle}/rss"
            xml = fetch_url(nitter_url, timeout=8)
            if xml and ("<item>" in xml or "<entry>" in xml):
                items = parse_rss_items(xml, name, role)
                if items:
                    results.extend(items[:2])
                    print(f"  âœ… {name}: {len(items)} æ¡ (via Nitter)")
                    fetched = True

        if not fetched:
            print(f"  âš ï¸ {name} (@{handle}): æ— æ³•æŠ“å–")

    print(f"  å…±è·å– Twitter åŠ¨æ€: {len(results)} æ¡")
    return results


def fetch_youtube_channels() -> list[dict]:
    """æŠ“å–æ‰€æœ‰ YouTube é¢‘é“çš„æœ€æ–°è§†é¢‘"""
    results = []
    print(f"â–¶  æŠ“å– YouTube é¢‘é“ ({len(YOUTUBE_CHANNELS)} ä¸ª)...")

    for ch in YOUTUBE_CHANNELS:
        url = f"https://www.youtube.com/feeds/videos.xml?channel_id={ch['channel_id']}"
        xml = fetch_url(url, timeout=10)
        if not xml:
            print(f"  âš ï¸ {ch['name']}: æ— æ³•æŠ“å–")
            continue

        items = []
        try:
            root = ET.fromstring(xml)
            ns = {
                "atom":  "http://www.w3.org/2005/Atom",
                "media": "http://search.yahoo.com/mrss/",
                "yt":    "http://www.youtube.com/xml/schemas/2015",
            }
            for entry in root.findall("atom:entry", ns)[:2]:
                title    = (entry.findtext("atom:title", "", ns) or "").strip()
                link_el  = entry.find("atom:link", ns)
                link     = link_el.get("href", "") if link_el is not None else ""
                pub      = (entry.findtext("atom:published", "", ns) or "").strip()
                desc_el  = entry.find(".//media:description", ns)
                desc     = clean_html(desc_el.text or "" if desc_el is not None else "", 200)

                if not title:
                    continue
                items.append({
                    "channel_name": ch["name"],
                    "channel_desc": ch["desc"],
                    "title":        title,
                    "title_zh":     translate_to_zh(title),
                    "summary":      desc,
                    "url":          link,
                    "published":    pub,
                    "type":         "youtube",
                })
        except ET.ParseError:
            pass

        if items:
            results.extend(items)
            print(f"  âœ… {ch['name']}: {len(items)} æ¡")
        else:
            print(f"  âš ï¸ {ch['name']}: è§£æå¤±è´¥")

        time.sleep(0.5)

    print(f"  å…±è·å– YouTube è§†é¢‘: {len(results)} æ¡")
    return results

# â”€â”€ è¯»å–æœ¬åœ°æ–°é—»æ•°æ® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_news(data_dir: Path, max_items: int = 20) -> list[dict]:
    """è¯»å– update_news.py äº§å‡ºçš„ latest-24h.json"""
    for fname in ["latest-24h.json", "latest.json", "snapshot.json"]:
        p = data_dir / fname
        if p.exists():
            try:
                data = json.loads(p.read_text(encoding="utf-8"))
                items = data.get("items", data if isinstance(data, list) else [])
                print(f"âœ… è¯»å–æ–°é—»: {p} ({len(items)} æ¡)")
                return items[:max_items]
            except Exception as e:
                print(f"âš ï¸ {p}: {e}")
    print("âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°æ–°é—»æ•°æ®")
    return []

# â”€â”€ æ„å»º HTML é‚®ä»¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _esc(text: str) -> str:
    """HTML è½¬ä¹‰"""
    return (str(text or "")
            .replace("&", "&amp;")
            .replace("<", "&lt;")
            .replace(">", "&gt;")
            .replace('"', "&quot;"))


def build_email(news_items: list[dict],
                tw_items:   list[dict],
                yt_items:   list[dict]) -> tuple[str, str]:
    """ç”Ÿæˆ (html, plain) é‚®ä»¶å†…å®¹"""

    tz8  = timezone(timedelta(hours=8))
    now  = datetime.now(tz8)
    date_str = now.strftime("%Yå¹´%mæœˆ%dæ—¥")
    time_str = now.strftime("%H:%M")

    # â”€â”€ æ–°é—» HTML â”€â”€
    news_rows = ""
    news_plain = ""
    for i, item in enumerate(news_items, 1):
        title = _esc(item.get("title_zh") or item.get("title") or "æ— æ ‡é¢˜")
        url   = item.get("url", "#")
        src   = _esc(item.get("source") or item.get("site_name") or "")
        news_rows += f"""
        <tr><td style="padding:10px 0;border-bottom:1px solid #f0f0f0;">
          <a href="{url}" style="color:#1a73e8;font-weight:600;font-size:14px;text-decoration:none;">{i}. {title}</a>
          {"<div style='color:#999;font-size:11px;margin-top:3px;'>ğŸ“° " + src + "</div>" if src else ""}
        </td></tr>"""
        news_plain += f"{i}. {item.get('title_zh') or item.get('title','')}\n   {url}\n\n"

    if not news_rows:
        news_rows = "<tr><td style='color:#999;padding:10px 0;'>ä»Šæ—¥æš‚æ— æ–°é—»æ•°æ®</td></tr>"
        news_plain = "ä»Šæ—¥æš‚æ— æ–°é—»æ•°æ®\n"

    # â”€â”€ Twitter HTML â”€â”€
    tw_rows = ""
    tw_plain = ""
    for item in tw_items:
        name    = _esc(item["expert_name"])
        role    = _esc(item["expert_role"])
        content = _esc(item["content"])
        zh      = _esc(item.get("content_zh", ""))
        url     = item.get("url", "#")

        # å¦‚æœç¿»è¯‘åªæ˜¯ "[è‹±] ..." å°±åªæ˜¾ç¤ºåŸæ–‡
        show_zh = zh and not zh.startswith("[è‹±]")

        tw_rows += f"""
        <tr><td style="padding:12px 0;border-bottom:1px solid #e8f4fd;">
          <div style="display:flex;align-items:center;margin-bottom:6px;">
            <span style="background:#1da1f2;color:#fff;border-radius:50%;width:32px;height:32px;display:inline-flex;align-items:center;justify-content:center;font-size:14px;margin-right:8px;">ğŸ¦</span>
            <div>
              <span style="font-weight:700;color:#1da1f2;font-size:14px;">{name}</span>
              <span style="color:#999;font-size:12px;margin-left:6px;">{role}</span>
            </div>
          </div>
          <div style="color:#333;font-size:13px;line-height:1.6;margin-left:40px;">{content}</div>
          {"<div style='color:#555;font-size:13px;line-height:1.6;margin-left:40px;margin-top:4px;background:#f0f7ff;padding:6px 8px;border-radius:4px;'>ğŸ‡¨ğŸ‡³ " + zh + "</div>" if show_zh else ""}
          {"<div style='margin:6px 0 0 40px;'><a href='" + url + "' style='color:#1da1f2;font-size:12px;'>æŸ¥çœ‹åŸæ–‡ â†’</a></div>" if url != "#" else ""}
        </td></tr>"""

        tw_plain += f"ğŸ¦ {item['expert_name']} ({item['expert_role']})\n"
        tw_plain += f"   {item['content']}\n"
        if url != "#":
            tw_plain += f"   {url}\n"
        tw_plain += "\n"

    if not tw_rows:
        tw_rows = """<tr><td style="color:#999;padding:10px 0;font-size:13px;">
            ä»Šæ—¥ Twitter ä¸“å®¶åŠ¨æ€æš‚æœªè·å–åˆ°ã€‚<br>
            <small>ï¼ˆTwitter RSS æºä¸ç¨³å®šï¼Œå¦‚æŒç»­æ— å†…å®¹å¯è€ƒè™‘é…ç½® Twitter APIï¼‰</small>
          </td></tr>"""
        tw_plain = "ä»Šæ—¥ Twitter ä¸“å®¶åŠ¨æ€æš‚æœªè·å–ã€‚\n"

    # â”€â”€ YouTube HTML â”€â”€
    yt_rows = ""
    yt_plain = ""
    for item in yt_items:
        ch    = _esc(item["channel_name"])
        desc  = _esc(item["channel_desc"])
        title = _esc(item["title"])
        title_zh = _esc(item.get("title_zh", ""))
        url   = item.get("url", "#")
        show_zh = title_zh and not title_zh.startswith("[è‹±]")

        yt_rows += f"""
        <tr><td style="padding:12px 0;border-bottom:1px solid #fff0f0;">
          <div style="margin-bottom:6px;">
            <span style="background:#ff0000;color:#fff;border-radius:4px;padding:2px 6px;font-size:11px;margin-right:6px;">â–¶ YouTube</span>
            <span style="font-weight:700;color:#ff0000;font-size:13px;">{ch}</span>
            <span style="color:#999;font-size:12px;margin-left:6px;">{desc}</span>
          </div>
          <a href="{url}" style="color:#333;font-weight:600;font-size:14px;text-decoration:none;">{title}</a>
          {"<div style='color:#555;font-size:13px;margin-top:4px;background:#fff5f5;padding:6px 8px;border-radius:4px;'>ğŸ‡¨ğŸ‡³ " + title_zh + "</div>" if show_zh else ""}
        </td></tr>"""

        yt_plain += f"â–¶ {item['channel_name']} - {item['title']}\n   {url}\n\n"

    if not yt_rows:
        yt_rows = "<tr><td style='color:#999;padding:10px 0;'>ä»Šæ—¥ YouTube è§†é¢‘æš‚æœªè·å–åˆ°ã€‚</td></tr>"
        yt_plain = "ä»Šæ—¥ YouTube è§†é¢‘æš‚æœªè·å–ã€‚\n"

    # â”€â”€ å®Œæ•´ HTML â”€â”€
    html = f"""<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>AI æ–°é—»é›·è¾¾æ—¥æŠ¥</title>
</head>
<body style="margin:0;padding:0;background:#f0f2f5;font-family:'PingFang SC','Microsoft YaHei',Arial,sans-serif;">
<div style="max-width:700px;margin:24px auto;background:#fff;border-radius:12px;overflow:hidden;box-shadow:0 2px 16px rgba(0,0,0,.1);">

  <!-- å¤´éƒ¨ -->
  <div style="background:linear-gradient(135deg,#1a73e8 0%,#0d47a1 100%);padding:32px;text-align:center;">
    <div style="font-size:32px;margin-bottom:8px;">ğŸ¤–</div>
    <div style="color:#fff;font-size:24px;font-weight:700;letter-spacing:2px;">AI æ–°é—»é›·è¾¾æ—¥æŠ¥</div>
    <div style="color:rgba(255,255,255,.75);font-size:14px;margin-top:8px;">{date_str} &nbsp;Â·&nbsp; {time_str} åŒ—äº¬æ—¶é—´ &nbsp;Â·&nbsp; æ¯æ—¥ç²¾é€‰</div>
  </div>

  <!-- ä»Šæ—¥ AI æ–°é—» -->
  <div style="padding:28px 32px;">
    <div style="font-size:18px;font-weight:700;color:#222;margin-bottom:16px;padding-bottom:10px;border-bottom:3px solid #1a73e8;">
      ğŸ“° ä»Šæ—¥ AI æ–°é—» <span style="font-size:14px;color:#999;font-weight:400;">({len(news_items)} æ¡)</span>
    </div>
    <table width="100%" cellpadding="0" cellspacing="0">{news_rows}</table>
  </div>

  <!-- Twitter ä¸“å®¶åŠ¨æ€ -->
  <div style="padding:28px 32px;background:#f7fbff;">
    <div style="font-size:18px;font-weight:700;color:#222;margin-bottom:4px;padding-bottom:10px;border-bottom:3px solid #1da1f2;">
      ğŸ¦ AI ä¸“å®¶ Twitter åŠ¨æ€ <span style="font-size:14px;color:#999;font-weight:400;">({len(tw_items)} æ¡)</span>
    </div>
    <div style="color:#999;font-size:12px;margin-bottom:14px;">
      è¿½è¸ªï¼š{" Â· ".join(e["name"] for e in TWITTER_EXPERTS)}
    </div>
    <table width="100%" cellpadding="0" cellspacing="0">{tw_rows}</table>
  </div>

  <!-- YouTube è§†é¢‘ -->
  <div style="padding:28px 32px;">
    <div style="font-size:18px;font-weight:700;color:#222;margin-bottom:4px;padding-bottom:10px;border-bottom:3px solid #ff0000;">
      â–¶ AI YouTube é¢‘é“ <span style="font-size:14px;color:#999;font-weight:400;">({len(yt_items)} æ¡)</span>
    </div>
    <div style="color:#999;font-size:12px;margin-bottom:14px;">
      é¢‘é“ï¼š{" Â· ".join(c["name"] for c in YOUTUBE_CHANNELS)}
    </div>
    <table width="100%" cellpadding="0" cellspacing="0">{yt_rows}</table>
  </div>

  <!-- åº•éƒ¨ -->
  <div style="background:#f5f5f5;padding:20px 32px;text-align:center;">
    <div style="color:#999;font-size:12px;line-height:1.8;">
      ç”± <strong>AI News Radar</strong> è‡ªåŠ¨ç”Ÿæˆ &nbsp;Â·&nbsp; GitHub Actions &nbsp;Â·&nbsp; æ¯æ—¥ 07:30 åŒ—äº¬æ—¶é—´<br>
      æ–°é—»æ¥æºï¼šå¤šä¸ª AI æƒå¨åª’ä½“ RSS &nbsp;Â·&nbsp; ä¸“å®¶åŠ¨æ€å®æ—¶æŠ“å–
    </div>
  </div>

</div>
</body>
</html>"""

    plain = f"""AI æ–°é—»é›·è¾¾æ—¥æŠ¥ â€” {date_str}
{"="*55}

ã€ä»Šæ—¥ AI æ–°é—» ({len(news_items)} æ¡)ã€‘
{news_plain}
{"="*55}

ã€AI ä¸“å®¶ Twitter åŠ¨æ€ ({len(tw_items)} æ¡)ã€‘
{tw_plain}
{"="*55}

ã€AI YouTube é¢‘é“ ({len(yt_items)} æ¡)ã€‘
{yt_plain}
{"="*55}
AI News Radar | æ¯æ—¥ 07:30 åŒ—äº¬æ—¶é—´è‡ªåŠ¨å‘é€
"""

    return html, plain

# â”€â”€ å‘é€é‚®ä»¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def send_email(html: str, plain: str) -> bool:
    server   = os.environ.get("SMTP_SERVER", "")
    port     = int(os.environ.get("SMTP_PORT", "587"))
    user     = os.environ.get("SENDER_EMAIL", "")
    password = os.environ.get("SMTP_PASSWORD", "")
    receiver = os.environ.get("RECEIVER_EMAIL", "")

    if not all([server, user, password, receiver]):
        missing = [k for k, v in {
            "SMTP_SERVER": server, "SENDER_EMAIL": user,
            "SMTP_PASSWORD": password, "RECEIVER_EMAIL": receiver
        }.items() if not v]
        print(f"âŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: {', '.join(missing)}")
        return False

    tz8   = timezone(timedelta(hours=8))
    today = datetime.now(tz8).strftime("%Y-%m-%d")

    msg = MIMEMultipart("alternative")
    msg["Subject"] = f"ğŸ¤– AI æ–°é—»é›·è¾¾æ—¥æŠ¥ Â· {today}"
    msg["From"]    = f"AI News Radar <{user}>"
    msg["To"]      = receiver
    msg.attach(MIMEText(plain, "plain", "utf-8"))
    msg.attach(MIMEText(html,  "html",  "utf-8"))

    # æ–¹æ¡ˆ1: STARTTLS
    try:
        print(f"ğŸ“¤ STARTTLS:{port} â†’ {receiver} ...")
        with smtplib.SMTP(server, port, timeout=30) as s:
            s.ehlo(); s.starttls(); s.ehlo()
            s.login(user, password)
            s.send_message(msg)
        print("âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
        return True
    except Exception as e:
        print(f"   STARTTLS å¤±è´¥: {e}")

    # æ–¹æ¡ˆ2: SSL
    try:
        print("ğŸ“¤ SSL:465 ...")
        ctx = ssl.create_default_context()
        with smtplib.SMTP_SSL(server, 465, context=ctx, timeout=30) as s:
            s.login(user, password)
            s.send_message(msg)
        print("âœ… é‚®ä»¶å‘é€æˆåŠŸ (SSL)ï¼")
        return True
    except Exception as e:
        print(f"   SSL:465 å¤±è´¥: {e}")

    print("âŒ æ‰€æœ‰ SMTP æ–¹æ¡ˆå‡å¤±è´¥")
    return False

# â”€â”€ ä¸»å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    print("ğŸš€ AI News Radar â€” é‚®ä»¶ç³»ç»Ÿ v3")
    print("=" * 55)

    # 1. è¯»å–æœ¬åœ°æ–°é—»
    news_items = load_news(DATA_DIR, max_items=20)

    # 2. å®æ—¶æŠ“å– Twitter ä¸“å®¶åŠ¨æ€
    tw_items = fetch_twitter_experts()

    # 3. å®æ—¶æŠ“å– YouTube é¢‘é“
    yt_items = fetch_youtube_channels()

    # 4. æ„å»ºé‚®ä»¶
    print("\nğŸ“§ æ„å»ºé‚®ä»¶å†…å®¹...")
    html_body, plain_body = build_email(news_items, tw_items, yt_items)
    print(f"   HTML: {len(html_body):,} å­—èŠ‚")

    # 5. å‘é€
    print("\nğŸ“¬ å‘é€é‚®ä»¶...")
    ok = send_email(html_body, plain_body)

    print("\n" + "=" * 55)
    print(f"{'âœ… å®Œæˆ' if ok else 'âŒ å¤±è´¥'} â€” "
          f"æ–°é—» {len(news_items)} æ¡ | Twitter {len(tw_items)} æ¡ | YouTube {len(yt_items)} æ¡")
    sys.exit(0 if ok else 1)


if __name__ == "__main__":
    main()
